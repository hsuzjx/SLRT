# configs/model/PatchModel.yaml

name: "PatchModel"

kps_idx:
  body: [
    0, 1, 3, 5, 7, 9, 91, 92, 93, 94, 95, 96, 97, 98, 99,
    100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,
    111, 2, 4, 6, 8, 10, 112, 113, 114, 115, 116, 117, 118,
    119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,
    130, 131, 132, 23, 26, 29, 33, 36, 39, 41, 43, 46, 48,
    53, 56, 59, 62, 65, 68, 71, 72, 73, 74, 75, 76, 77, 79,
    80, 81
  ]
  left: [
    0, 1, 3, 5, 7, 9, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,
    101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111
  ]
  right: [
    0, 2, 4, 6, 8, 10, 112, 113, 114, 115, 116, 117, 118, 119,
    120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132
  ]
  face: [
    23, 26, 29, 33, 36, 39, 41, 43, 46, 48, 53, 56, 59, 62, 65,
    68, 71, 72, 73, 74, 75, 76, 77, 79, 80, 81
  ]

network:
  share_classifier: true
  ResNet18:
    pretrained: true
    model_dir: ../.pretrained_models
  conv1d:
    input_size: 128
    hidden_size: 128
    conv_type: 2
    use_bn: false
  BiLSTM:
    rnn_type: 'LSTM'
    input_size: 1024
    hidden_size: 1024
    num_layers: 2
    bidirectional: true
  head_cfg: {
    'body_visual_head': {
      'input_size': 512,
      'hidden_size': 1024,
      'ff_size': 4096,
      'pe': True,
      'ff_kernelsize': [ 3, 3 ]
    },
    'fuse_visual_head': {
      'input_size': 2048,
      'hidden_size': 1024,
      'ff_size': 4096,
      'pe': True,
      'ff_kernelsize': [ 3, 3 ]
    },
    'left_visual_head': {
      'input_size': 1024,
      'hidden_size': 1024,
      'ff_size': 4096,
      'pe': True,
      'ff_kernelsize': [ 3, 3 ]
    },
    'right_visual_head': {
      'input_size': 1024,
      'hidden_size': 1024,
      'ff_size': 4096,
      'pe': True,
      'ff_kernelsize': [ 3, 3 ]
    }
  }

#loss_fn:
#  CTCLoss:
#    blank: 0
#    zero_infinity: false
#    reduction: 'sum'
#  KLDivLoss:
#    reduction: "mean"
#  SeqKD:
#    T: 8

optimizer:
  Adam:
    lr: 0.001
    weight_decay: 0.001
    betas: [ 0.9, 0.998 ]
    eps: 1e-8

lr_scheduler:
  MultiStepLR:
    milestones: [ 20, 35 ]
    gamma: 0.2
    last_epoch: -1
  CosineAnnealingLR:
    T_max: 100
    eta_min: 0
    last_epoch: -1
    verbose: "deprecated"

callback:
  EarlyStopping:
    patience: 50
    verbose: true
    check_finite: false
  ModelCheckpoint:
    filename: 'top_{epoch}'
    save_last: true
    save_top_k: 3
    verbose: true

test_param: true